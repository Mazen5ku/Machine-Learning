{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # \"Header\"\n",
    "\n",
    "<p style=\"text-align: left;  font-size:18pt; LINE-HEIGHT:30px;\">\n",
    "    <span style=\"float: left\">\n",
    "     Technische Hochschule Ingolstadt<br>\n",
    "     Prof. Dr. Sören Gröttrup <br>\n",
    "     Laura Dietl\n",
    "    </span>\n",
    "    <span style=\"float: right;\">\n",
    "       Machine Learning 1<br>\n",
    "        <span style=\"float: right;\">WS 24/25</span>\n",
    "    </span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # \"Header Aufgabenblatt\"\n",
    "<br>\n",
    "<p style=\"text-align: center;  font-size:18pt; LINE-HEIGHT:30px;\">\n",
    "     <span style=\"font-weight: bold;\">Aufgabenblatt 4</span><br>\n",
    "     Themen: Imputing, Neuronale Netze in PyTorch, Backpropagation<br>\n",
    "     Abgabetermin: 29.11.2024, 23:59 Uhr <br>\n",
    "     Punkte: 31\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Namen:** Elkhan Alimatov (ela0016), Mazen Zidan (maz4669) , Hamdah Mariyam (ham2119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Imputing - Dealing with Missing Values - Ein Weg mit fehlenden Werten umzugehen\n",
    "\n",
    "Bei einer Variablen in einem Datensatz kann es durchaus vorkommen, dass für einige Beobachtungen die Werte nicht bekannt sind. Zum Beispiel kann es sein, dass ein Kunde in einem Online-Shop sein Profil nicht vollkommen ausfüllt, so dass die Handynummer, das Alter oder gewisse Interessen nicht bekannt sind. Die Variable Handynummer in diesem Datensatz wäre dann für den jeweiligen Kunden nicht befüllt.\n",
    "\n",
    "Einige Algorithmen des Maschinellen Lernens können mit diesen fehlenden Werten nicht umgehen. Damit man einen Algorithmus trotzdem auf einen Datensatz mit einer Variable mit fehlenden Werten anwenden kann, gibt es einige einfache Strategien.\n",
    "\n",
    "**Ignoriere die fehlenden Werte (Omission)**\n",
    "\n",
    "Die einfachste und radikalste Methode ist die fehlenden Werte für das Training zu entfernen. Dies kann man auf zwei Arten machen:\n",
    "1. Entfernen der Variable aus dem Trainingsdatensatz. Z.B. im obigen Beispiel die Variable Handynummer nicht als Input-Variable zu nehmen.\n",
    "1. Entfernen aller Zeilen im Datensatz, die einen fehlenden Wert in einer zu berücksichtigen Variable aufweisen. Z.B. im obigen Beispiel alle Kunden ohne Handynummer aus dem Tainingsdatensatz ausschließen.\n",
    "\n",
    "**Imputing**\n",
    "\n",
    "Anstatt die fehlenden Werte zu entfernen, kann man auch versuchen, diese mit \"geeigneten\" Werten aufzufüllen. Dafür gibt es mehrere einfache Methoden.\n",
    "1. _Constant Value Imputing_ Wenn der Kontext es hergibt, können alle fehlenden Werte einer Variablen mit einem Standardwert aufgefüllt werden. \n",
    "    1. Bei einer kategoriellen Variable können z.B. die fehlenden Werte mit 'unknown' oder 'other' ersetzt werden. Wenn z.B. bei einem Kunden der Wert der Variable \"Intresse an Sportartikeln\" fehlt, kann dieser die neue Ausprägung \"unknown\" erhalten.\n",
    "    1. Wenn bei einer numerischen Variablen nur positive Werte auftreten können, wie z.B. bei der Variable \"Anzahl an Käufe\", kann der fehlende Wert sinnvollerweise mit 0 ersetzt werden.\n",
    "1. _Mean/Median/Mode Imputing_ Die fehlenden Werte einer Variablen werden durch deren arithmetisches Mittel, den Median oder den Modus ersetzt. Diese repräsentiern \"typische\" Werte für die Variable.\n",
    "    1. Bei kategoriellen Variablen werden die fehlenden Werte durch die am häufigsten auftretende Ausprägung ersetzt (den Modus).\n",
    "    1. Bei numerischen Variablen werden die fehlenden Werte mit dem arithmetischen Mittel oder dem Median der anderen Ausprägungen der Variable ersetzt.\n",
    "\n",
    "Dies sind nur die einfachen und univariaten Methoden des Imputing. Weitere multivariate Imputing-Verfahren wie die Benutzung von Vorhersagemodellen werden in der Vorlesung besprochen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1.1 (Vor- & Nachteile der Imputing-Verfahren) [6 Punkte]\n",
    "Betrachten Sie die oben genannten Omission und Imputing-Verfahren. Welche Vor- und Nachteile könnten diese Verfahren jeweils haben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Antwort zu Aufgabe 1.1**\n",
    "\n",
    "+ Constant Value Imputing\n",
    "\n",
    "   + Vorteile: Standardwerte wie unknown oder 0 sind leicht zu implementieren.\n",
    "             Die Anzahl der Beobachtungen bleibt gleich.\n",
    "             Man kann das Ganze anpassen, je nachdem, was gerade gebraucht wird.\n",
    "\n",
    "   + Nachteile: Es kann zu Verzerrungen kommen: Eingefügte Standardwerte können die Daten verfälschen.\n",
    "              Wenn man einen Standardwert oft benutzt, kann das Ergebnis ungenauer werden.\n",
    "\n",
    "\n",
    "+ Mean/Median/Mode Imputing\n",
    "\n",
    "   + Vorteile: Arithmetisches Mittel, Median und Modus repräsentieren oft gut den Gesamtdatensatz.\n",
    "             Die Methode ist flexibel, weil es anwendbar auf numerische und kategorische Variablen ist.\n",
    "             Die Berechnung ist einfach.\n",
    "\n",
    "   + Nachteile: Ersetzte Werte können die Realität nicht immer korrekt widerspiegeln.\n",
    "              Mean Imputation kann bei nicht normal verteilten Daten zu unrealistischen Werten führen.\n",
    "              Alle eingefügten Werte sind gleich, was die Streuung der Daten verfälschen kann.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neuronale Netze mit PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten\n",
    "\n",
    "Der Datensatz `gt.csv` (zu finden auf der Moodle-Seite des Praktikums (https://moodle.thi.de/course/view.php?id=6824)) ist eine etwas abgewandelte und bearbeitete Version des Datensatzes aus dem UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Gas+Turbine+CO+and+NOx+Emission+Data+Set). \n",
    "\n",
    "Der Datensatz enthält 36733 Messpunkte von je 11 Sensoren einer Gasturbine im Nordwesten der Türkei. Die einzelnen Werte sind dabei jeweils Aggregate (Summe oder Mittelwert) über eine Stunde. Die Sensordaten diesen dazu die Gasemissionender Turbine, insb. CO und NOx (NO & NO2), zu messen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisierung der Daten\n",
    "Hier finden Sie bereits geschriebenen Code, der die Daten einliest und in Trainings- und Testdaten splittet. __Bitte verändern Sie diesen Code nicht__, sondern fügen Ihren Code unterhalb ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5878</td>\n",
       "      <td>1018.7</td>\n",
       "      <td>83.675</td>\n",
       "      <td>3.5758</td>\n",
       "      <td>23.979</td>\n",
       "      <td>1086.2</td>\n",
       "      <td>549.83</td>\n",
       "      <td>134.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32663</td>\n",
       "      <td>81.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2932</td>\n",
       "      <td>1018.3</td>\n",
       "      <td>84.235</td>\n",
       "      <td>3.5709</td>\n",
       "      <td>23.951</td>\n",
       "      <td>1086.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.67</td>\n",
       "      <td>11.892</td>\n",
       "      <td>0.44784</td>\n",
       "      <td>82.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.9045</td>\n",
       "      <td>1018.4</td>\n",
       "      <td>84.858</td>\n",
       "      <td>3.5828</td>\n",
       "      <td>23.990</td>\n",
       "      <td>1086.5</td>\n",
       "      <td>550.19</td>\n",
       "      <td>135.10</td>\n",
       "      <td>12.042</td>\n",
       "      <td>0.45144</td>\n",
       "      <td>83.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.7436</td>\n",
       "      <td>1018.3</td>\n",
       "      <td>85.434</td>\n",
       "      <td>3.5808</td>\n",
       "      <td>23.911</td>\n",
       "      <td>1086.5</td>\n",
       "      <td>550.17</td>\n",
       "      <td>135.03</td>\n",
       "      <td>11.990</td>\n",
       "      <td>0.23107</td>\n",
       "      <td>82.505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.7516</td>\n",
       "      <td>1017.8</td>\n",
       "      <td>85.182</td>\n",
       "      <td>3.5781</td>\n",
       "      <td>23.917</td>\n",
       "      <td>1085.9</td>\n",
       "      <td>550.00</td>\n",
       "      <td>134.67</td>\n",
       "      <td>11.910</td>\n",
       "      <td>0.26747</td>\n",
       "      <td>82.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36728</th>\n",
       "      <td>3.6268</td>\n",
       "      <td>1028.5</td>\n",
       "      <td>93.200</td>\n",
       "      <td>3.1661</td>\n",
       "      <td>19.087</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>541.59</td>\n",
       "      <td>109.08</td>\n",
       "      <td>10.411</td>\n",
       "      <td>10.99300</td>\n",
       "      <td>89.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36729</th>\n",
       "      <td>4.1674</td>\n",
       "      <td>1028.6</td>\n",
       "      <td>94.036</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>19.016</td>\n",
       "      <td>1037.6</td>\n",
       "      <td>542.28</td>\n",
       "      <td>108.79</td>\n",
       "      <td>10.344</td>\n",
       "      <td>11.14400</td>\n",
       "      <td>88.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36730</th>\n",
       "      <td>5.4820</td>\n",
       "      <td>1028.5</td>\n",
       "      <td>95.219</td>\n",
       "      <td>3.3128</td>\n",
       "      <td>18.857</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>543.48</td>\n",
       "      <td>107.81</td>\n",
       "      <td>10.462</td>\n",
       "      <td>11.41400</td>\n",
       "      <td>96.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36731</th>\n",
       "      <td>5.8837</td>\n",
       "      <td>1028.7</td>\n",
       "      <td>94.200</td>\n",
       "      <td>3.9831</td>\n",
       "      <td>23.563</td>\n",
       "      <td>1076.9</td>\n",
       "      <td>550.11</td>\n",
       "      <td>131.41</td>\n",
       "      <td>11.771</td>\n",
       "      <td>3.31340</td>\n",
       "      <td>64.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36732</th>\n",
       "      <td>6.0392</td>\n",
       "      <td>1028.8</td>\n",
       "      <td>94.547</td>\n",
       "      <td>3.8752</td>\n",
       "      <td>22.524</td>\n",
       "      <td>1067.9</td>\n",
       "      <td>548.23</td>\n",
       "      <td>125.41</td>\n",
       "      <td>11.462</td>\n",
       "      <td>11.98100</td>\n",
       "      <td>109.240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36733 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      4.5878  1018.7  83.675  3.5758  23.979  1086.2  549.83  134.67     NaN   \n",
       "1      4.2932  1018.3  84.235  3.5709  23.951  1086.1     NaN  134.67  11.892   \n",
       "2      3.9045  1018.4  84.858  3.5828  23.990  1086.5  550.19  135.10  12.042   \n",
       "3      3.7436  1018.3  85.434  3.5808  23.911  1086.5  550.17  135.03  11.990   \n",
       "4      3.7516  1017.8  85.182  3.5781  23.917  1085.9  550.00  134.67  11.910   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "36728  3.6268  1028.5  93.200  3.1661  19.087  1037.0  541.59  109.08  10.411   \n",
       "36729  4.1674  1028.6  94.036  3.1923  19.016  1037.6  542.28  108.79  10.344   \n",
       "36730  5.4820  1028.5  95.219  3.3128  18.857  1038.0  543.48  107.81  10.462   \n",
       "36731  5.8837  1028.7  94.200  3.9831  23.563  1076.9  550.11  131.41  11.771   \n",
       "36732  6.0392  1028.8  94.547  3.8752  22.524  1067.9  548.23  125.41  11.462   \n",
       "\n",
       "             CO      NOX  \n",
       "0       0.32663   81.952  \n",
       "1       0.44784   82.377  \n",
       "2       0.45144   83.776  \n",
       "3       0.23107   82.505  \n",
       "4       0.26747   82.028  \n",
       "...         ...      ...  \n",
       "36728  10.99300   89.172  \n",
       "36729  11.14400   88.849  \n",
       "36730  11.41400   96.147  \n",
       "36731   3.31340   64.738  \n",
       "36732  11.98100  109.240  \n",
       "\n",
       "[36733 rows x 11 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "file_path = \"Daten Blatt 4/gt.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and feature\n",
    "y = data.NOX\n",
    "X = data.drop(columns=[\"CO\", \"NOX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.1 (Datenaufbereitung) [4 Punkte]\n",
    "\n",
    "Untersuchen Sie den Datensatz der Sensordaten einer Gasturbine `gt.csv`.\n",
    "\n",
    "1. Welche Variablen weisen fehlende Werte auf?\n",
    "1. Ersetzen Sie diese mittels eines sinnvollen oben genannten Imputing-Verfahrens. _Hinweis_: Sie können dafür die Funktion `SimpleImputer()` aus dem `sklearn` Paket verwenden. Informieren Sie sich vorher über deren Funktionsweise und Optionen.\n",
    "1. Was für ein Vorverarbeitungsschritt scheint noch sinnvoll zu sein? Führen Sie diesen ggf. auch durch.\n",
    "\n",
    "_Achtung_: Behalten Sie bei Ihren Datenoperationen die gegebene Aufteilung in Training- und Testdaten bei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lösung Aufgabe 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Die variablen CDP und TAT haben fehlende werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:  AT         0\n",
      "AP         0\n",
      "AH         0\n",
      "AFDP       0\n",
      "GTEP       0\n",
      "TIT        0\n",
      "TAT     6605\n",
      "TEY        0\n",
      "CDP     4426\n",
      "CO         0\n",
      "NOX        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "\n",
    "print(\"Missing Values: \", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AT      0\n",
       "AP      0\n",
       "AH      0\n",
       "AFDP    0\n",
       "GTEP    0\n",
       "TIT     0\n",
       "TAT     0\n",
       "TEY     0\n",
       "CDP     0\n",
       "CO      0\n",
       "NOX     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teil 2\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create imputers for missing values\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation for the missing columns (TAT and CDP)\n",
    "data['TAT'] = mean_imputer.fit_transform(data[['TAT']])\n",
    "data['CDP'] = mean_imputer.fit_transform(data[['CDP']])\n",
    "\n",
    "# Verify if missing values are filled\n",
    "mv_after_imput = data.isnull().sum()\n",
    "\n",
    "mv_after_imput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die daten können auch skaliert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.762362</td>\n",
       "      <td>0.871052</td>\n",
       "      <td>0.401627</td>\n",
       "      <td>-0.451875</td>\n",
       "      <td>-0.377702</td>\n",
       "      <td>0.272119</td>\n",
       "      <td>0.591406</td>\n",
       "      <td>0.074502</td>\n",
       "      <td>1.743825e-15</td>\n",
       "      <td>-0.904182</td>\n",
       "      <td>1.426499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.801920</td>\n",
       "      <td>0.809164</td>\n",
       "      <td>0.440351</td>\n",
       "      <td>-0.458207</td>\n",
       "      <td>-0.384376</td>\n",
       "      <td>0.266417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074502</td>\n",
       "      <td>-1.625139e-01</td>\n",
       "      <td>-0.850611</td>\n",
       "      <td>1.462891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.854113</td>\n",
       "      <td>0.824636</td>\n",
       "      <td>0.483432</td>\n",
       "      <td>-0.442831</td>\n",
       "      <td>-0.375081</td>\n",
       "      <td>0.289227</td>\n",
       "      <td>0.649445</td>\n",
       "      <td>0.102033</td>\n",
       "      <td>-1.526101e-02</td>\n",
       "      <td>-0.849020</td>\n",
       "      <td>1.582687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.875718</td>\n",
       "      <td>0.809164</td>\n",
       "      <td>0.523263</td>\n",
       "      <td>-0.445415</td>\n",
       "      <td>-0.393909</td>\n",
       "      <td>0.289227</td>\n",
       "      <td>0.646221</td>\n",
       "      <td>0.097551</td>\n",
       "      <td>-6.630868e-02</td>\n",
       "      <td>-0.946415</td>\n",
       "      <td>1.473852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.874644</td>\n",
       "      <td>0.731804</td>\n",
       "      <td>0.505837</td>\n",
       "      <td>-0.448904</td>\n",
       "      <td>-0.392479</td>\n",
       "      <td>0.255012</td>\n",
       "      <td>0.618814</td>\n",
       "      <td>0.074502</td>\n",
       "      <td>-1.448436e-01</td>\n",
       "      <td>-0.930328</td>\n",
       "      <td>1.433006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AT        AP        AH      AFDP      GTEP       TIT       TAT  \\\n",
       "0 -1.762362  0.871052  0.401627 -0.451875 -0.377702  0.272119  0.591406   \n",
       "1 -1.801920  0.809164  0.440351 -0.458207 -0.384376  0.266417  0.000000   \n",
       "2 -1.854113  0.824636  0.483432 -0.442831 -0.375081  0.289227  0.649445   \n",
       "3 -1.875718  0.809164  0.523263 -0.445415 -0.393909  0.289227  0.646221   \n",
       "4 -1.874644  0.731804  0.505837 -0.448904 -0.392479  0.255012  0.618814   \n",
       "\n",
       "        TEY           CDP        CO       NOX  \n",
       "0  0.074502  1.743825e-15 -0.904182  1.426499  \n",
       "1  0.074502 -1.625139e-01 -0.850611  1.462891  \n",
       "2  0.102033 -1.526101e-02 -0.849020  1.582687  \n",
       "3  0.097551 -6.630868e-02 -0.946415  1.473852  \n",
       "4  0.074502 -1.448436e-01 -0.930328  1.433006  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teil 3\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Display the first few rows of the standardized dataset to verify\n",
    "standardized_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.2 (Training in PyTorch) [12 Punkte]\n",
    "Erstellen Sie ein Feedforward Neuronales Netz auf dem in der vorherigen Aufgabe 2.1 aufbereiteten Datensatz mittels PyTorch, welches basierend auf den anderen Sensordaten die NOx-Emission vorhersagen kann. Benutzen Sie dabei den L2-Loss als Kostenfunktion. Welchen RMSE erzielt Ihr trainiertes Neuronales Netz, wenn Sie dies auf die Testdaten (`X_test, y_test`) anwenden?\n",
    "\n",
    "In dieser Aufgabe gibt es keinerlei weitere Vorgabe an das zu implementierende Modell. Sie sind in der Struktur des Netzes und dem Vorgehen frei. Um aber die volle Punkzahl zu erhalten, müssen Sie ein sinnvolles Netz trainieren, d.h. die Optimierung mittels Gradient Descent konvergiert, kein Overfitting etc.. Versuchen Sie auch den RMSE so klein wie möglich zu bekommen.\n",
    "\n",
    "_Hinweis_: Anstatt Ihre aufbereiteten Daten zu nehmen, können Sie auch schon vorverarbeitete Daten einlesen durch den Befehl `data = torch.load(\"Daten Blatt 4/gt_prepared\")`\n",
    "Dieser liest ein Dictionary ein, in welchem Sie Tensoren der Trainings- und Testdaten finden. Der Befehl `data.keys()` zeigt Ihnen die Key-Namen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lösung der Aufgabe 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as net\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_data', 'train_target', 'test_data', 'test_target'])\n",
      "torch.Size([7347, 1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"Daten Blatt 4/gt_prepared\")\n",
    "print(data.keys())\n",
    "X_train_prep = data[\"train_data\"]\n",
    "y_train_prep = data[\"train_target\"].view(-1, 1)\n",
    "X_test_prep = data[\"test_data\"]\n",
    "y_test_prep = data[\"test_target\"].view(-1, 1)\n",
    "print(y_test_prep.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b1c2d98570>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Hyper-Parameters\n",
    "n_iterations = 100\n",
    "\n",
    "\n",
    "random_seed = 70\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(9, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Train set - loss: 66.091 | Test set - Loss: 65.696\n",
      "Iteration 2, Train set - loss: 65.759 | Test set - Loss: 65.268\n",
      "Iteration 3, Train set - loss: 65.33 | Test set - Loss: 64.644\n",
      "Iteration 4, Train set - loss: 64.704 | Test set - Loss: 63.656\n",
      "Iteration 5, Train set - loss: 63.711 | Test set - Loss: 61.988\n",
      "Iteration 6, Train set - loss: 62.036 | Test set - Loss: 59.046\n",
      "Iteration 7, Train set - loss: 59.078 | Test set - Loss: 53.671\n",
      "Iteration 8, Train set - loss: 53.674 | Test set - Loss: 43.925\n",
      "Iteration 9, Train set - loss: 43.869 | Test set - Loss: 29.38\n",
      "Iteration 10, Train set - loss: 29.233 | Test set - Loss: 20.501\n",
      "Iteration 11, Train set - loss: 20.381 | Test set - Loss: 17.609\n",
      "Iteration 12, Train set - loss: 17.532 | Test set - Loss: 15.396\n",
      "Iteration 13, Train set - loss: 15.329 | Test set - Loss: 13.699\n",
      "Iteration 14, Train set - loss: 13.64 | Test set - Loss: 12.454\n",
      "Iteration 15, Train set - loss: 12.402 | Test set - Loss: 11.577\n",
      "Iteration 16, Train set - loss: 11.531 | Test set - Loss: 10.974\n",
      "Iteration 17, Train set - loss: 10.932 | Test set - Loss: 10.557\n",
      "Iteration 18, Train set - loss: 10.519 | Test set - Loss: 10.262\n",
      "Iteration 19, Train set - loss: 10.226 | Test set - Loss: 10.042\n",
      "Iteration 20, Train set - loss: 10.008 | Test set - Loss: 9.868\n",
      "Iteration 21, Train set - loss: 9.835 | Test set - Loss: 9.722\n",
      "Iteration 22, Train set - loss: 9.689 | Test set - Loss: 9.595\n",
      "Iteration 23, Train set - loss: 9.562 | Test set - Loss: 9.481\n",
      "Iteration 24, Train set - loss: 9.447 | Test set - Loss: 9.377\n",
      "Iteration 25, Train set - loss: 9.341 | Test set - Loss: 9.28\n",
      "Iteration 26, Train set - loss: 9.243 | Test set - Loss: 9.19\n",
      "Iteration 27, Train set - loss: 9.151 | Test set - Loss: 9.106\n",
      "Iteration 28, Train set - loss: 9.065 | Test set - Loss: 9.028\n",
      "Iteration 29, Train set - loss: 8.984 | Test set - Loss: 8.953\n",
      "Iteration 30, Train set - loss: 8.908 | Test set - Loss: 8.883\n",
      "Iteration 31, Train set - loss: 8.835 | Test set - Loss: 8.816\n",
      "Iteration 32, Train set - loss: 8.766 | Test set - Loss: 8.753\n",
      "Iteration 33, Train set - loss: 8.701 | Test set - Loss: 8.693\n",
      "Iteration 34, Train set - loss: 8.639 | Test set - Loss: 8.636\n",
      "Iteration 35, Train set - loss: 8.58 | Test set - Loss: 8.582\n",
      "Iteration 36, Train set - loss: 8.524 | Test set - Loss: 8.53\n",
      "Iteration 37, Train set - loss: 8.47 | Test set - Loss: 8.481\n",
      "Iteration 38, Train set - loss: 8.419 | Test set - Loss: 8.434\n",
      "Iteration 39, Train set - loss: 8.371 | Test set - Loss: 8.389\n",
      "Iteration 40, Train set - loss: 8.325 | Test set - Loss: 8.347\n",
      "Iteration 41, Train set - loss: 8.281 | Test set - Loss: 8.306\n",
      "Iteration 42, Train set - loss: 8.239 | Test set - Loss: 8.267\n",
      "Iteration 43, Train set - loss: 8.198 | Test set - Loss: 8.23\n",
      "Iteration 44, Train set - loss: 8.159 | Test set - Loss: 8.194\n",
      "Iteration 45, Train set - loss: 8.122 | Test set - Loss: 8.16\n",
      "Iteration 46, Train set - loss: 8.086 | Test set - Loss: 8.127\n",
      "Iteration 47, Train set - loss: 8.052 | Test set - Loss: 8.095\n",
      "Iteration 48, Train set - loss: 8.019 | Test set - Loss: 8.064\n",
      "Iteration 49, Train set - loss: 7.987 | Test set - Loss: 8.035\n",
      "Iteration 50, Train set - loss: 7.956 | Test set - Loss: 8.007\n",
      "Iteration 51, Train set - loss: 7.927 | Test set - Loss: 7.979\n",
      "Iteration 52, Train set - loss: 7.898 | Test set - Loss: 7.953\n",
      "Iteration 53, Train set - loss: 7.87 | Test set - Loss: 7.927\n",
      "Iteration 54, Train set - loss: 7.844 | Test set - Loss: 7.902\n",
      "Iteration 55, Train set - loss: 7.818 | Test set - Loss: 7.878\n",
      "Iteration 56, Train set - loss: 7.793 | Test set - Loss: 7.855\n",
      "Iteration 57, Train set - loss: 7.769 | Test set - Loss: 7.833\n",
      "Iteration 58, Train set - loss: 7.745 | Test set - Loss: 7.811\n",
      "Iteration 59, Train set - loss: 7.723 | Test set - Loss: 7.79\n",
      "Iteration 60, Train set - loss: 7.701 | Test set - Loss: 7.769\n",
      "Iteration 61, Train set - loss: 7.679 | Test set - Loss: 7.749\n",
      "Iteration 62, Train set - loss: 7.659 | Test set - Loss: 7.73\n",
      "Iteration 63, Train set - loss: 7.639 | Test set - Loss: 7.712\n",
      "Iteration 64, Train set - loss: 7.619 | Test set - Loss: 7.693\n",
      "Iteration 65, Train set - loss: 7.6 | Test set - Loss: 7.676\n",
      "Iteration 66, Train set - loss: 7.582 | Test set - Loss: 7.659\n",
      "Iteration 67, Train set - loss: 7.564 | Test set - Loss: 7.642\n",
      "Iteration 68, Train set - loss: 7.546 | Test set - Loss: 7.626\n",
      "Iteration 69, Train set - loss: 7.529 | Test set - Loss: 7.61\n",
      "Iteration 70, Train set - loss: 7.513 | Test set - Loss: 7.595\n",
      "Iteration 71, Train set - loss: 7.497 | Test set - Loss: 7.58\n",
      "Iteration 72, Train set - loss: 7.481 | Test set - Loss: 7.565\n",
      "Iteration 73, Train set - loss: 7.466 | Test set - Loss: 7.551\n",
      "Iteration 74, Train set - loss: 7.451 | Test set - Loss: 7.537\n",
      "Iteration 75, Train set - loss: 7.436 | Test set - Loss: 7.524\n",
      "Iteration 76, Train set - loss: 7.422 | Test set - Loss: 7.51\n",
      "Iteration 77, Train set - loss: 7.408 | Test set - Loss: 7.497\n",
      "Iteration 78, Train set - loss: 7.395 | Test set - Loss: 7.485\n",
      "Iteration 79, Train set - loss: 7.381 | Test set - Loss: 7.473\n",
      "Iteration 80, Train set - loss: 7.369 | Test set - Loss: 7.461\n",
      "Iteration 81, Train set - loss: 7.356 | Test set - Loss: 7.449\n",
      "Iteration 82, Train set - loss: 7.344 | Test set - Loss: 7.437\n",
      "Iteration 83, Train set - loss: 7.331 | Test set - Loss: 7.426\n",
      "Iteration 84, Train set - loss: 7.32 | Test set - Loss: 7.415\n",
      "Iteration 85, Train set - loss: 7.308 | Test set - Loss: 7.404\n",
      "Iteration 86, Train set - loss: 7.297 | Test set - Loss: 7.394\n",
      "Iteration 87, Train set - loss: 7.286 | Test set - Loss: 7.384\n",
      "Iteration 88, Train set - loss: 7.275 | Test set - Loss: 7.373\n",
      "Iteration 89, Train set - loss: 7.264 | Test set - Loss: 7.364\n",
      "Iteration 90, Train set - loss: 7.254 | Test set - Loss: 7.354\n",
      "Iteration 91, Train set - loss: 7.244 | Test set - Loss: 7.345\n",
      "Iteration 92, Train set - loss: 7.234 | Test set - Loss: 7.335\n",
      "Iteration 93, Train set - loss: 7.224 | Test set - Loss: 7.326\n",
      "Iteration 94, Train set - loss: 7.215 | Test set - Loss: 7.317\n",
      "Iteration 95, Train set - loss: 7.205 | Test set - Loss: 7.309\n",
      "Iteration 96, Train set - loss: 7.196 | Test set - Loss: 7.3\n",
      "Iteration 97, Train set - loss: 7.187 | Test set - Loss: 7.292\n",
      "Iteration 98, Train set - loss: 7.178 | Test set - Loss: 7.283\n",
      "Iteration 99, Train set - loss: 7.17 | Test set - Loss: 7.275\n",
      "Iteration 100, Train set - loss: 7.161 | Test set - Loss: 7.267\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_iterations + 1):\n",
    "    \n",
    "    # --- Training ----#\n",
    "    model.train()\n",
    "    optimizer.zero_grad()                          \n",
    "    y_pred = model(X_train_prep)                  \n",
    "    train_loss = F.mse_loss(y_pred, y_train_prep.float())\n",
    "    train_loss.backward()\n",
    "    optimizer.step()                                \n",
    "    \n",
    "    \n",
    "    # ----- Evaluation metrics train ---- #\n",
    "    train_losses.append(np.sqrt(train_loss.item()))\n",
    "    \n",
    "    # ----- Test Prediction & Evaluation -------------#\n",
    "    model.eval()\n",
    "    y_test_pred = model(X_test_prep)\n",
    "    \n",
    "    # Loss\n",
    "    test_loss = F.mse_loss(y_test_pred, y_test_prep.float())\n",
    "    test_losses.append(np.sqrt(test_loss.item()))\n",
    "    \n",
    "    # Print Validation criteria\n",
    "    print(f'''Iteration {epoch}, Train set - loss: {round(np.sqrt(train_loss.item()), 3)} | Test set - Loss: {round(np.sqrt(test_loss.item()), 3)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE on test set: 7.26734611881089\n"
     ]
    }
   ],
   "source": [
    "final_rmse = np.sqrt(test_loss.item())\n",
    "print(f'Final RMSE on test set: {final_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(test_losses, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parameter eines Neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3.1 [3 Punkte]\n",
    "Gegeben ist das in PyTorch definierte Neuronale Netze\n",
    "   \n",
    "![fc_nn](Bilder/fc_nn.png)\n",
    "\n",
    "Bestimmen Sie  die Anzahl zu optimierender Parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antwort zu Aufgabe 3.1\n",
    "\n",
    "Optimierungsparameter eines Netzes sind Gewichte und Bias.\n",
    "Das Netzwerk besteht aus 4 \"Layers\": Input, 2 Hidden und Output Layers.\n",
    "\n",
    "1. Hidden Layer besteht aus 8 Neuronen und jede Neuron hat 9 Input Data.\n",
    "   Gewichte: 9x8 = 72\n",
    "   Bias: 8\n",
    "   Anzahl der Parameter: 72 + 8 = 80\n",
    "2. Hidden Layer besteht aus 6 Neuronen und jede Neuron hat 8 Input Data.\n",
    "   Gewichte: 8x6 = 48\n",
    "   Bias: 6\n",
    "   Anzahl der Parameter: 48 + 6 = 54\n",
    "\n",
    "3. Output Layer besteht aus einem Neuron und hat 6 Input Data.\n",
    "   Gewichte: 6x1 = 6\n",
    "   Bias: 1\n",
    "   Anzahl der Parameter: 6 + 1 = 7\n",
    "\n",
    "Anzahl der Parameter des Netzes: 80 + 54 + 7 = 141.\n",
    "Das neuronale Netzwerk hat insgesamt 141 zu optimierende Parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4.1 [6 Punkte]\n",
    "Gegeben ist folgendes Neuronale Netze\n",
    "   \n",
    "![fc_nn](Bilder/NN_backpropagation.png)\n",
    "\n",
    "mit folgenden Gewichtsmatrizen \n",
    "\n",
    "$$W^{(1)}=\\left(\\begin{array}{cc} \n",
    "1 & 1\\\\ \n",
    "2 & -2\n",
    "\\end{array}\\right)\n",
    "\\quad\\text{und}\\quad\n",
    "W^{(2)}=\\left(\\begin{array}{c} \n",
    "1\\\\ \n",
    "-1\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "und Biases\n",
    "\n",
    "$$b^{(1)}=\\left(\\begin{array}{c} \n",
    "-1\\\\ \n",
    "0\n",
    "\\end{array}\\right)\n",
    "\\quad\\text{und}\\quad\n",
    "b^{(2)}= -4$$\n",
    "\n",
    "Die Aktivierungsfunktionen sind folgende:\n",
    "* Hidden Layer: ReLU\n",
    "* Output Layer: Identität\n",
    "\n",
    "Als Verlustfunktion wird der L2-Loss genommen\n",
    "$$L(y,a)=\\frac12(y-a)^2$$\n",
    "\n",
    "__Aufgabe:__\n",
    "1. Führen Sie ein Update der Parameter mittels Backpropagation durch für den Datenpunkt\n",
    "$$x=\\left(\\begin{array}{c} \n",
    "1\\\\ \n",
    "1 \n",
    "\\end{array}\\right) $$\n",
    "mit wahrem Zielwert \n",
    "$$ y= -1$$\n",
    "und der Lernrate $\\alpha=0.1$ durch.\n",
    "1. Wie groß ist der neue Verlust nach dem Parameterupdate? Vergleichen Sie das mit dem Verlust vor durchführung des Updates der Parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Forward\n",
    "$$\\ z_1^{(1)} = 2 \\$$\n",
    "$$\\ z_2^{(1)} = -1 \\$$\n",
    "$$\\ a_1^{(1)} = 2 \\$$\n",
    "$$\\ a_2^{(1)} = 0 \\$$\n",
    "$$\\ z_1^{(2)} = -1 \\$$\n",
    "$$ \\hat y = -2 $$\n",
    "$$\\text { loss }=\\frac{1}{2}(y-\\hat{y})^2=\\frac{1}{2}(-1+2)^2=0,5$$\n",
    "# Ableitungen\n",
    "$$\\begin{aligned}  \\frac{d}{d L} L(y, \\hat{y}) =-2+1=-1\n",
    "\\end{aligned}$$\n",
    "$$\\frac{d}{d z_0} \\hat{y}=1$$\n",
    "$$\\frac{d}{d w_{11}^{(2)}} z^{(2)}=a_1^{(1)}=2$$\n",
    "$$\\frac{d}{d w_{21}^{(2)}} z^{(2)}=a_2^{(1)}=0$$\n",
    "$$\\frac{d}{d b_{1}^{(2)}} z^{(2)}=1$$\n",
    "$$\\frac{d}{d a_{1}^{(1)}} z^{(2)}=w_{22}^{(2)}=1$$\n",
    "$$\\frac{d}{d a_{2}^{(1)}} z^{(2)}=w_{21}^{(2)}=-1$$\n",
    "$$\\frac{d}{d z_{1}^{(1)}} a_1^{(1)}=1$$\n",
    "$$\\frac{d}{d w_{11}^{(1)}} z_1^{(1)}=x_1=1$$\n",
    "$$\\frac{d}{d w_{21}^{(1)}} z_1^{(1)}=x_2=1$$\n",
    "$$\\frac{d}{d b_{2}^{(1)}} z^{(1)}=1$$\n",
    "$$\\frac{d}{d z_{2}^{(1)}} a_2^{(1)}=1$$\n",
    "$$\\frac{d}{d w_{12}^{(1)}} z_2^{(1)}=x_1=1$$\n",
    "$$\\frac{d}{d w_{22}^{(1)}} z_2^{(1)}=x_2=1$$\n",
    "# Updates\n",
    "$$\\ b_{(1, new)}^{(2)} = b_1^{(2)} - \\alpha \\frac{d}{d b_{1}^{(2)}}  L = -4 - 0.1*-1 = -3.9\\$$\n",
    "$$\\ b_{(1, new)}^{(1)} = -0.9 $$\n",
    "$$\\ b_{(2, new)}^{(1)} = -0.1 $$\n",
    "$$\\ w_{(11, new)}^{(2)} = 1.2 $$\n",
    "$$\\ w_{(21, new)}^{(2)} = -1 $$\n",
    "$$\\ w_{(11, new)}^{(1)} = 1.1 $$\n",
    "$$\\ w_{(21, new)}^{(1)} = 2.1 $$\n",
    "$$\\ w_{(12, new)}^{(1)} = 0.9 $$\n",
    "$$\\ w_{(22, new)}^{(1)} = -2.1 $$\n",
    "# forward with updated parameters\n",
    "$$\\ z_{(1, new)}^{(1)} = 2.3 \\$$\n",
    "$$\\ z_{(2, new)}^{(1)} = -1.3 \\$$\n",
    "$$\\ a_{(1, new)}^{(1)} = 2.3 \\$$\n",
    "$$\\ a_{(2, new)}^{(1)} = 0 \\$$\n",
    "$$\\ z_{(1, new)}^{(2)} = -1.14 \\$$\n",
    "$$ \\hat y_{(new)} = -1.14 $$\n",
    "$$\\text { loss }=\\frac{1}{2}(y-\\hat{y_{(new)}})^2=\\frac{1}{2}(-1+1.14)^2=0,0098$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
